{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "\n",
    "- Stream larger-than-memory data through a pipeline\n",
    "- Composable thanks to the iterator protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My favorite \"feature\" of pandas is that it's written in Python.\n",
    "Python has great language-level features for handling streams of data\n",
    "that may not fit in memory.\n",
    "This can be a useful pre-processing step to reading the data into a DataFrame or\n",
    "NumPy array.\n",
    "You can get quite far using just the builtin data structures as David Beazley proves in [this PyData keynote](https://www.youtube.com/watch?v=lyDLAutA88s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "from itertools import islice, takewhile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import dask.dataframe as dd\n",
    "from toolz import partition_all, partitionby\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 10\n",
    "sns.set(context='talk')\n",
    "plt.style.use(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beer Reviews Dataset\n",
    "\n",
    "- A review is a list of lines\n",
    "- Each review line is formated like `meta/field: value`\n",
    "- Reviews are separated by blank lines (i.e. the line is just `'\\n'`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford has a [dataset on beer reviews](https://snap.stanford.edu/data/web-BeerAdvocate.html). The raw file is too large for me to include, but I split off a couple subsets for us to work with.\n",
    "\n",
    "Pandas can't read this file natively, but we have Python!\n",
    "We'll use Python to parse the raw file and tranform it into a tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"data/beer-raw-small.txt.gz\", \"r\") as f:\n",
    "    print(f.read(1500).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full compressed raw dataset is about 500MB, so reading it all into memory might not be pleasent (we're working with a small subset that would fit in memory, but pretend it didn't).\n",
    "Fortunately, Python's iterator protocol and generators make dealing with large streams of data pleasent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing a solution\n",
    "\n",
    "Let's build a solution together. I'll provide some guidance as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a handle to the data\n",
    "f = gzip.open(\"data/beer-raw-small.txt.gz\", \"rt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually you'd use a context manager like `with gzip.open(...) as f`, but for debugging, it's OK to do it this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Tasks\n",
    "\n",
    "1. split the raw text stream into individual reviews\n",
    "2. transform each individual review into a data container\n",
    "3. combine a chunk of transformed individual reviews into a collection\n",
    "4. store the chunk to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the first review using [`takewhile`](https://docs.python.org/3/library/itertools.html#itertools.takewhile) till the first `'\\n'`.\n",
    "`takewhile` scans a stream, returning each item (line) until it hits the sentinal value it's looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import takewhile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.seek(0);  # make the cell idempotent\n",
    "first = list(takewhile(lambda x: x != '\\n', f))\n",
    "first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" data-title=\"Format Review\">\n",
    "  <h1><i class=\"fa fa-tasks\" aria-hidden=\"true\"></i> Exercise: Format Review</h1>\n",
    "</div>\n",
    "<p>Write a function `format_review` that converts an item like `first` into a dict</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will have one entry per line, where the are the stuff to the left of the colon and the values are the stuff to the right.\n",
    "For example, the first line would be\n",
    "\n",
    "`'beer/name: Sausa Weizen\\n',` => `'beer/name': 'Sausa Weizen'`\n",
    "\n",
    "Make sure to clean up the line endings too.\n",
    "\n",
    "- Hint: Check out the [python string methods](https://docs.python.org/3/library/stdtypes.html#string-methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your function against `expected` by evaluating the next cell.\n",
    "If you get a failure, adjust your `format_review` until it passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from typing import List, Dict\n",
    "\n",
    "f.seek(0);  # make the cell idempotent\n",
    "review = list(takewhile(lambda x: x != '\\n', f))\n",
    "\n",
    "\n",
    "def format_review(review: List[str]) -> Dict[str, str]:\n",
    "    \"\"\"Your code goes below\"\"\"\n",
    "    \n",
    "\n",
    "class TestFormat(unittest.TestCase):\n",
    "    maxDiff = None\n",
    "\n",
    "    def test_format_review(self):\n",
    "        result = format_review(review)\n",
    "        expected = {\n",
    "            'beer/ABV': '5.00',\n",
    "            'beer/beerId': '47986',\n",
    "            'beer/brewerId': '10325',\n",
    "            'beer/name': 'Sausa Weizen',\n",
    "            'beer/style': 'Hefeweizen',\n",
    "            'review/appearance': '2.5',\n",
    "            'review/aroma': '2',\n",
    "            'review/overall': '1.5',\n",
    "            'review/palate': '1.5',\n",
    "            'review/profileName': 'stcules',\n",
    "            'review/taste': '1.5',\n",
    "            'review/text': 'A lot of foam. But a lot.\\tIn the smell some banana, and then lactic and tart. Not a good start.\\tQuite dark orange in color, with a lively carbonation (now visible, under the foam).\\tAgain tending to lactic sourness.\\tSame for the taste. With some yeast and banana.\\t\\t',\n",
    "            'review/time': '1234817823'\n",
    "        }\n",
    "        self.assertEqual(result, expected)\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromModule(TestFormat())\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/groupby_format_review.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that optional argument to split, which controls the number of splits made; If a review text had contained a literal `': '`, we'd be in trouble since it'd get split again.\n",
    "\n",
    "Make sure you executed the above solution cell twice (first to load, second to execute) as we'll be using that `format_review` function down below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To a DataFrame\n",
    "\n",
    "Assuming we've processed many reviews into a list, we'll then build up a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = [format_review(first)]  # imagine a list of many reviews\n",
    "\n",
    "col_names = {\n",
    "    'beer/ABV': 'abv',\n",
    "    'beer/beerId': 'beer_id',\n",
    "    'beer/brewerId': 'brewer_id',\n",
    "    'beer/name': 'beer_name',\n",
    "    'beer/style': 'beer_style',\n",
    "    'review/appearance': 'review_appearance',\n",
    "    'review/aroma': 'review_aroma',\n",
    "    'review/overall': 'review_overall',\n",
    "    'review/palate': 'review_palate',\n",
    "    'review/profileName': 'profile_name',\n",
    "    'review/taste': 'review_taste',\n",
    "    'review/text': 'text',\n",
    "    'review/time': 'time'\n",
    "}\n",
    "df = pd.DataFrame(r)\n",
    "numeric = ['abv', 'review_appearance', 'review_aroma',\n",
    "           'review_overall', 'review_palate', 'review_taste']\n",
    "df = (df.rename(columns=col_names)\n",
    "        .replace('', np.nan))\n",
    "df[numeric] = df[numeric].astype(float)\n",
    "df['time'] = pd.to_datetime(df.time.astype(int), unit='s')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, writing that as a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_dataframe(reviews):\n",
    "    col_names = {\n",
    "        'beer/ABV': 'abv',\n",
    "        'beer/beerId': 'beer_id',\n",
    "        'beer/brewerId': 'brewer_id',\n",
    "        'beer/name': 'beer_name',\n",
    "        'beer/style': 'beer_style',\n",
    "        'review/appearance': 'review_appearance',\n",
    "        'review/aroma': 'review_aroma',\n",
    "        'review/overall': 'review_overall',\n",
    "        'review/palate': 'review_palate',\n",
    "        'review/profileName': 'profile_name',\n",
    "        'review/taste': 'review_taste',\n",
    "        'review/text': 'text',\n",
    "        'review/time': 'time'\n",
    "    }\n",
    "    df = pd.DataFrame(list(reviews))\n",
    "    numeric = ['abv', 'review_appearance', 'review_aroma',\n",
    "               'review_overall', 'review_palate', 'review_taste']\n",
    "    df = (df.rename(columns=col_names)\n",
    "            .replace('', np.nan))\n",
    "    df[numeric] = df[numeric].astype(float)\n",
    "    df['time'] = pd.to_datetime(df.time.astype(int), unit='s')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full pipeline\n",
    "\n",
    "1. `file -> review_lines : List[str]`\n",
    "2. `review_lines -> reviews : Dict[str, str]`\n",
    "3. `reviews -> DataFrames`\n",
    "4. `DataFrames -> CSV`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full pipeline would look something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import partition_all, partitionby\n",
    "\n",
    "\n",
    "BATCH_SIZE = 100  # Number of reviews to process per chunk\n",
    "                  # Intentionally small for demostration    \n",
    "\n",
    "\n",
    "with gzip.open(\"data/beer-raw-small.txt.gz\", \"rt\") as f:\n",
    "\n",
    "    # Filter out a null byte at the end\n",
    "    lines = (x for x in f if not x.startswith('\\x00'))\n",
    "    \n",
    "    review_lines_and_newlines = partitionby(lambda x: x == '\\n', lines)\n",
    "    # that goes [review, \\n, review, \\n, ...]\n",
    "    # so filter out the newlines\n",
    "    review_lines = filter(lambda x: x != ('\\n',), review_lines_and_newlines)\n",
    "    \n",
    "    # generator expression to go from List[str] -> Dict[str, str]\n",
    "    reviews = (format_review(x) for x in review_lines)\n",
    "    \n",
    "    # `reviews` yields one dict per review.\n",
    "    # Won't fit in memory, so do `BATCH_SIZE` per chunk\n",
    "    chunks = partition_all(BATCH_SIZE, reviews)\n",
    "    dfs = (as_dataframe(chunk) for chunk in chunks)\n",
    "    os.makedirs(\"data/beer/\", exist_ok=True)\n",
    "\n",
    "    # the first time we read from disk\n",
    "    for i, df in enumerate(dfs):\n",
    "        df.to_csv(\"data/beer/chunk_%s.csv.gz\" % i, index=False,\n",
    "                  compression=\"gzip\")\n",
    "        print(i, end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This runs comfortably in memory. At any given time, we only have `BATCH_SIZE` reviews in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Aside on [Dask](http://dask.pydata.org/en/latest/)\n",
    "\n",
    "> Dask is a flexible parallel computing library for analytic computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset is in random order, but I wanted to select an interesting subset for us to work on: all the reviews by the top 100 reviewers.\n",
    "\n",
    "You might know enough pandas now to figure out the top-100 reviewers by count.\n",
    "Do a `value_counts` , select the top 100, then select the index.\n",
    "\n",
    "```python\n",
    "top_reviewers = df.profile_name.value_counts().nlargest(100).index\n",
    "```\n",
    "\n",
    "Recall that `value_counts` will be a `Series` where the index is each unique `profile_name` and the values is the count of reviews for that profile.\n",
    "We use `nlargets(100)` to get the 100 largest values, and `.index` to get the actual profile names. \n",
    "\n",
    "But that assumes we have a `df` containing the full dataset in memory.\n",
    "My laptop can't load the entire dataset though (recall that we're working with a subset today).\n",
    "\n",
    "It wouldn't be *that* hard to write a custom solution in python or pandas using chunking like we did up above.\n",
    "We'd split our task into parts\n",
    "\n",
    "- read a chunk\n",
    "- compute `chunk.profile_name.value_counts()`\n",
    "- store that intermediate `value_counts` in a global container\n",
    "\n",
    "Once we've processed each chunk, our final steps are to\n",
    "\n",
    "- merge each `value_counts` chunk by summing\n",
    "- filter to the top 100\n",
    "\n",
    "This pattern of processing chunks independently (map) and combining the results into a smaller output (reduce) is common, and doing it manually gets old.\n",
    "Dask can help out here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collections, Tasks, Schedulers\n",
    "\n",
    "![](http://dask.pydata.org/en/latest/_images/collections-schedulers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask has several components, so it can be hard to succinctly describe the library.\n",
    "For our purposes, we'll view it as providing \"big dataframes\".\n",
    "In reality, \"big dataframes\" means a collection of many smaller pandas `DataFrame`s, with algorithms for operating on them in parallel and combining the results.\n",
    "You write python code that's quite similar to regular NumPy and pandas, only instead of execting immediatly a *task graph* (DAG for Directect Acylic Graph) is created.\n",
    "Dask knows how to *schedule* the DAG to execute on some workers (the several cores or your laptop, or an entire distributed cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dd.read_csv(\"data/beer/chunk*.csv.gz\", compression=\"gzip\", blocksize=None,\n",
    "                parse_dates=['time'])\n",
    "a.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That API should look familiar to you, now that you're experienced pandas users.\n",
    "We swap out `pd` for `dd`, and stuff mostly just works.\n",
    "Occasionally, you'll have a `dask`-specific thing like `blocksize` (number of bytes per smaller dataframe) that don't apply to pandas, which assumes things fit in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_reviewers = a.profile_name.value_counts().nlargest(100).index\n",
    "top_reviewers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't really done any work yet. `dask` has just built the DAG representing the computation it will perform when needed.\n",
    "We can directly visualize DAGs that aren't too large with the `.visualize` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the DAG built up by dask with the `.visualize` method.\n",
    "I've done this ahead of time, since then library that draws the graphs isn't part of the required packages.\n",
    "\n",
    "```python\n",
    "top_reviewers.visualize()\n",
    "```\n",
    "![](mydask.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_reviewers.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `top_reviewers` as a boolean mask, just like in regular pandas.\n",
    "\n",
    "```python\n",
    ">>> a[a.profile_name.isin(top_reviewers.compute())].to_csv(\"data/subset.csv.gz\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to pandas\n",
    "\n",
    "I've provided the reviews by the top 100 reviewers.\n",
    "We'll use it for talking about groupby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/subset.csv.gz\", compression=\"gzip\",\n",
    "                 parse_dates=['time'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Namespaces\n",
    "\n",
    "Pandas has been expanding its use of namespaces (or accessors) on `DataFrame` to group together related methods. This also limits the number of methods direclty attached to `DataFrame` itself, which can be overwhelming.\n",
    "\n",
    "Currently, we have these namespaces:\n",
    "\n",
    "- `.str`: defined on `Series` and `Index`es containing strings (object dtype)\n",
    "- `.dt`: defined on `Series` with `datetime` or `timedelta` dtype\n",
    "- `.cat`: defined on `Series` and `Indexes` with `category` dtype\n",
    "- `.plot`: defined on `Series` and `DataFrames`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" data-title=\"Reviews by Hour\">\n",
    "  <h1><i class=\"fa fa-tasks\" aria-hidden=\"true\"></i> Exercise: Reviews by Hour</h1>\n",
    "</div>\n",
    "\n",
    "<p>Make a barplot of the count of reviews by hour of the day.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hint: Use the `.dt` namespace to get the `hour` component of a `datetime`\n",
    "- Hint: We've seen `Series.value_counts` for getting the count of each value\n",
    "- Hint: Use `.sort_index` to make sure the data is ordered by hour, not count\n",
    "- Hint: Use the [`.plot`](http://pandas.pydata.org/pandas-docs/stable/api.html#plotting) namespace to get a `bar` chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/groupby_03.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" data-title=\"Pale Ales\">\n",
    "  <h1><i class=\"fa fa-tasks\" aria-hidden=\"true\"></i> Exercise: Pale Ales</h1>\n",
    "</div>\n",
    "<p>\n",
    "Make a variable `pale_ales` that filters `df` to just rows where `beer_style` contains the string `'pale ale'` (ignoring case)\n",
    "</p>\n",
    "- Hint: Use the `df.beer_style.str` namespace and find a method for checking whether a string contains another string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/groupby_04.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groupby operations come up in a lot of contexts.\n",
    "At its root, groupby about doing an operation on many subsets of the data, each of which shares something in common.\n",
    "The components of a groupby operation are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components of a groupby\n",
    "\n",
    "1. **split** a table into groups\n",
    "2. **apply** a function to each group\n",
    "3. **combine** the results into a single DataFrame or Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas the `split` step looks like\n",
    "\n",
    "```python\n",
    "df.groupby( grouper )\n",
    "```\n",
    "\n",
    "`grouper` can be many things\n",
    "\n",
    "- Series (or string indicating a column in `df`)\n",
    "- function (to be applied on the index)\n",
    "- dict : groups by *values*\n",
    "- `levels=[ names of levels in a MultiIndex ]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split\n",
    "\n",
    "Break a table into smaller logical tables according to some rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = df.groupby(\"beer_name\")\n",
    "gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't really done any actual work yet, but pandas knows what it needs to know to break the larger `df` into many smaller pieces, one for each distinct `beer_name`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply & Combine\n",
    "\n",
    "To finish the groupby, we apply a method to the groupby object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_cols = ['review_appearance', 'review_aroma', 'review_overall',\n",
    "               'review_palate', 'review_taste']\n",
    "\n",
    "df.groupby('beer_name')[review_cols].agg('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the function we applied was `'mean'`.\n",
    "Pandas has implemented cythonized versions of certain common methods like mean, sum, etc.\n",
    "You can also pass in regular functions like `np.mean`.\n",
    "\n",
    "In terms of split, apply, combine, split was `df.groupby('beer_name')`. \n",
    "We apply the `mean` function by passing in `'mean'`.\n",
    "Finally, by using the `.agg` method (for aggregate) we tell pandas to combine the results with one output row per group.\n",
    "\n",
    "You can also pass in regular functions like `np.mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('beer_name')[review_cols].agg(np.mean).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, [certain methods](http://pandas.pydata.org/pandas-docs/stable/api.html#id35) have been attached to `Groupby` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('beer_name')[review_cols].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" data-title=\"Highest Variance\">\n",
    "  <h1><i class=\"fa fa-tasks\" aria-hidden=\"true\"></i> Exercise: Highest Variance</h1>\n",
    "</div>\n",
    "\n",
    "<p>Find the `beer_style`s with the greatest variance in `abv`.</p>\n",
    "\n",
    "- hint: `.std` calculates the standard deviation (`.var` for variance), and is available on `GroupBy` objects like `gr.abv`.\n",
    "- hint: use `.sort_values` to sort a Series by the values (it took us a while to come up with that name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/groupby_abv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `.agg` output shape\n",
    "\n",
    "The output shape is determined by the grouper, data, and aggregation\n",
    "\n",
    "- Grouper: Controls the output index\n",
    "    * single grouper -> Index\n",
    "    * array-like grouper -> MultiIndex\n",
    "- Subject (Groupee): Controls the output data values\n",
    "    * single column -> Series (or DataFrame if multiple aggregations)\n",
    "    * multiple columns -> DataFrame\n",
    "- Aggregation: Controls the output columns\n",
    "    * single aggfunc -> Index in the colums\n",
    "    * multiple aggfuncs -> MultiIndex in the columns (Or 1-D Index if groupee is 1-D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We'll go into MultiIndexes in a bit, but for know, think of them as regular Indexes with multiple levels (columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single grouper, single groupee, single aggregation\n",
    "df.groupby('beer_style').review_overall.agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple groupers, multiple groupee, single aggregation\n",
    "df.groupby(['brewer_id', 'beer_name'])[review_cols].agg(['mean', 'min', 'max', 'std', 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" data-title=\"Rating by length\">\n",
    "  <h1><i class=\"fa fa-tasks\" aria-hidden=\"true\"></i> Exercise: Rating by length</h1>\n",
    "</div>\n",
    "\n",
    "<p>Plot the relationship between review length (number of characters) and average `reveiw_overall`.</p>\n",
    "\n",
    "- Hint: use `.plot(style='k.')`\n",
    "- We've grouped by columns so far, you can also group by any series with the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/groupby_00.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" data-title=\"Reviews by Length\">\n",
    "  <h1><i class=\"fa fa-tasks\" aria-hidden=\"true\"></i> Exercise: Reviews by Length</h1>\n",
    "</div>\n",
    "\n",
    "<p>Find the relationship between review length (number of **words** and average `reveiw_overall`.)</p>\n",
    "\n",
    "- Hint: You can pass a [regular expression](https://docs.python.org/3/howto/regex.html#matching-characters) to any of the `.str` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/groupby_00b.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" data-title=\"Rating by number of Reviews\">\n",
    "  <h1><i class=\"fa fa-tasks\" aria-hidden=\"true\"></i> Exercise: Rating by number of Reviews</h1>\n",
    "</div>\n",
    "\n",
    "<p>Find the relationship between the number of reviews for a beer and the average `review_overall`.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/groupby_01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "\n",
    "A *transform* is a function whose output is the same shape as the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a groupby has three steps: split, apply, combine.\n",
    "So far, all of the functions we've applied have been *aggregations*: the rule for \"combine\" is one row per group.\n",
    "\n",
    "You can use `Groupby.transform` when you have an operation that should be done *groupwise*, but the result should be the same shape.\n",
    "For example, suppose we wanted to normalize each reviewer's scores by their average score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define demean(v: array) -> array\n",
    "def demean(v):\n",
    "    return v - v.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just calling `demean` on the entire Series will noramilze by the *global* average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demean(df.review_overall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's demean each individual's reviews by their own average.\n",
    "This could be useful if, for example, you were building a recommendation system.\n",
    "A rating of 4 from someone's whose average is 2 is in some sense more meaningful that a 4 from someone who always gives 4s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = df.groupby(\"profile_name\")[review_cols].transform(demean)\n",
    "normalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used `.transform` because the desired output was the same shape as the input.\n",
    "Just like `.agg` informs pandas that you want `1 input group → 1 output row`, the `.transform` method informs pandas that you want `1 input row → 1 output row`.\n",
    "\n",
    "`.transform` operates on each column independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" data-title=\"Personal Trend?\">\n",
    "  <h1><i class=\"fa fa-tasks\" aria-hidden=\"true\"></i> Exercise: Personal Trend?</h1>\n",
    "</div>\n",
    "\n",
    "<p>Do reviewer's `review_overall` trend over a person's time reviewing?</p>\n",
    "\n",
    "Hint: Need an indictor that tracks which review this is for that person. That is, we need a cumulative count of reviews per person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/groupby_02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General `.apply`\n",
    "\n",
    "We've seen `.agg` for outputting 1 row per group, and `.transform` for outputting 1 row per input row.\n",
    "\n",
    "The final kind of function application is `.apply`.\n",
    "This can do pretty much whatever you want.\n",
    "We'll see an example in a later notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- We used Python's iterator protocol to transform the raw data to a table\n",
    "- We saw how Dask could handle larger-than-memory data with a familiar API\n",
    "- We used groupby to analyze data by subsets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
